{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c7a013",
   "metadata": {},
   "source": [
    "# CSV File Verification Notebook\n",
    "\n",
    "This notebook helps verify the correctness of the generated CSV files by comparing data from two sources:\n",
    "1. **CSV files** generated by `generate_csv_from_data.py`\n",
    "2. **Original JSON files** containing VLM captions and GPT-4o evaluations\n",
    "\n",
    "## Purpose\n",
    "- Verify that VLM captions were correctly extracted and aligned\n",
    "- Confirm that GPT-4o evaluations and scores match between sources\n",
    "- Ensure data integrity during the CSV generation process\n",
    "\n",
    "## Usage\n",
    "1. Set the parameters (video_id, chunk_id, model_name) in the designated cell\n",
    "2. Run all cells to perform the verification\n",
    "3. Review the comparison results at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55afc9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, Tuple\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f8e147",
   "metadata": {},
   "source": [
    "## üîß Configuration Parameters\n",
    "\n",
    "Set the test parameters below to verify specific data points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e8850ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ Test Parameters:\n",
      "   Video ID: 74GSMhR6oI0\n",
      "   Chunk ID: 0\n",
      "   Model: vila-1.5\n",
      "üìÇ Directories configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# ===== CONFIGURATION PARAMETERS =====\n",
    "# Modify these parameters to test different data points\n",
    "\n",
    "TEST_VIDEO_ID = \"74GSMhR6oI0\"  # Example video ID\n",
    "TEST_CHUNK_ID = 0               # Example chunk ID  \n",
    "TEST_MODEL_NAME = \"vila-1.5\"    # Example model name\n",
    "\n",
    "# File paths\n",
    "CSV_OUTPUT_DIR = \"csv_output\"\n",
    "CAPTIONS_DIR = \"reports/minseok_prompt_audio_x_caption_sorted\"\n",
    "EVALUATIONS_DIR = \"reports/caption_full_comparison_minseok_audio_x_caption_sorted_completeness\"\n",
    "\n",
    "print(f\"üéØ Test Parameters:\")\n",
    "print(f\"   Video ID: {TEST_VIDEO_ID}\")\n",
    "print(f\"   Chunk ID: {TEST_CHUNK_ID}\")\n",
    "print(f\"   Model: {TEST_MODEL_NAME}\")\n",
    "print(f\"üìÇ Directories configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc98df7",
   "metadata": {},
   "source": [
    "## üìÑ Load CSV File Data\n",
    "\n",
    "Load the generated CSV files and examine their structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f075314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loading: csv_output/vlm_captions.csv\n",
      "üìÑ Loading: csv_output/gpt4o_evaluations.csv\n",
      "‚úÖ CSV files loaded successfully!\n",
      "   üìä Captions CSV: 747 rows\n",
      "   üìä Evaluations CSV: 747 rows\n",
      "\n",
      "üìã Captions CSV Columns: ['video_id', 'chunk_id', 'start_time', 'end_time', 'vila-1.5', 'nvila', 'cosmos_reason1', 'qwen3-vl-30b-a3b-instruct', 'gemini-2.5-pro']\n",
      "üìã Evaluations CSV Columns: ['video_id', 'chunk_id', 'start_time', 'end_time', 'vila-1.5', 'vila-1.5_score', 'nvila', 'nvila_score', 'cosmos_reason1', 'cosmos_reason1_score', 'qwen3-vl-30b-a3b-instruct', 'qwen3-vl-30b-a3b-instruct_score']\n",
      "\n",
      "üîç Sample Captions Data:\n",
      "      video_id  chunk_id  start_time  end_time  \\\n",
      "0  74GSMhR6oI0         0         0.0      20.0   \n",
      "1  74GSMhR6oI0         1        20.0      40.0   \n",
      "\n",
      "                                            vila-1.5  \\\n",
      "0  A man is talking about a soccer game and holdi...   \n",
      "1  A woman wearing an orange jersey and black sho...   \n",
      "\n",
      "                                               nvila  \\\n",
      "0  <0.0>   <18.02>  A cartoon of a dog is shown n...   \n",
      "1  <20.02>   <38.01>  A woman is standing on a so...   \n",
      "\n",
      "                                      cosmos_reason1  \\\n",
      "0  <think>\\nOkay, let's break this down. The user...   \n",
      "1  <20.02> <24.02> A goalkeeper wearing an orange...   \n",
      "\n",
      "                           qwen3-vl-30b-a3b-instruct  \\\n",
      "0  <0> <2> A television screen displays an advert...   \n",
      "1  <20> <22> A player in a red jersey with the nu...   \n",
      "\n",
      "                                      gemini-2.5-pro  \n",
      "0  <0> <5> A promotional video for a sports brand...  \n",
      "1  <20> <22> A close-up shot of a soccer player's...  \n",
      "\n",
      "üîç Sample Evaluations Data:\n",
      "      video_id  chunk_id  start_time  end_time  \\\n",
      "0  74GSMhR6oI0         0         0.0      20.0   \n",
      "1  74GSMhR6oI0         1        20.0      40.0   \n",
      "\n",
      "                                            vila-1.5  vila-1.5_score  \\\n",
      "0  The test caption is severely lacking in comple...               3   \n",
      "1  The test caption is severely lacking in comple...               3   \n",
      "\n",
      "                                               nvila  nvila_score  \\\n",
      "0  The test caption is severely lacking in comple...            3   \n",
      "1  The test caption is severely lacking in comple...            3   \n",
      "\n",
      "                                      cosmos_reason1  cosmos_reason1_score  \\\n",
      "0  The test caption is severely lacking in comple...                     3   \n",
      "1  The test caption is severely lacking in comple...                     3   \n",
      "\n",
      "                           qwen3-vl-30b-a3b-instruct  \\\n",
      "0  The test caption covers most of the key elemen...   \n",
      "1  The test caption focuses on a sequence of even...   \n",
      "\n",
      "   qwen3-vl-30b-a3b-instruct_score  \n",
      "0                                8  \n",
      "1                                3  \n"
     ]
    }
   ],
   "source": [
    "# Load CSV files\n",
    "def load_csv_files():\n",
    "    \"\"\"Load both CSV files and return DataFrames.\"\"\"\n",
    "    \n",
    "    captions_csv = Path(CSV_OUTPUT_DIR) / \"vlm_captions.csv\"\n",
    "    evaluations_csv = Path(CSV_OUTPUT_DIR) / \"gpt4o_evaluations.csv\"\n",
    "    \n",
    "    if not captions_csv.exists():\n",
    "        raise FileNotFoundError(f\"Captions CSV not found: {captions_csv}\")\n",
    "    if not evaluations_csv.exists():\n",
    "        raise FileNotFoundError(f\"Evaluations CSV not found: {evaluations_csv}\")\n",
    "    \n",
    "    print(f\"üìÑ Loading: {captions_csv}\")\n",
    "    captions_df = pd.read_csv(captions_csv)\n",
    "    \n",
    "    print(f\"üìÑ Loading: {evaluations_csv}\")\n",
    "    evaluations_df = pd.read_csv(evaluations_csv)\n",
    "    \n",
    "    print(f\"‚úÖ CSV files loaded successfully!\")\n",
    "    print(f\"   üìä Captions CSV: {len(captions_df)} rows\")\n",
    "    print(f\"   üìä Evaluations CSV: {len(evaluations_df)} rows\")\n",
    "    \n",
    "    return captions_df, evaluations_df\n",
    "\n",
    "# Load the CSV data\n",
    "captions_df, evaluations_df = load_csv_files()\n",
    "\n",
    "# Display basic info about the CSV files\n",
    "print(f\"\\nüìã Captions CSV Columns: {list(captions_df.columns)}\")\n",
    "print(f\"üìã Evaluations CSV Columns: {list(evaluations_df.columns)}\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\nüîç Sample Captions Data:\")\n",
    "print(captions_df.head(2))\n",
    "print(f\"\\nüîç Sample Evaluations Data:\")\n",
    "print(evaluations_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da40ca0d",
   "metadata": {},
   "source": [
    "## üìÅ Load Original JSON File Data\n",
    "\n",
    "Load the original JSON files to compare against the CSV data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d101133e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Loading caption file: reports/minseok_prompt_audio_x_caption_sorted/vila-1.5/74GSMhR6oI0/vlm_captions_74GSMhR6oI0.json\n",
      "üìÑ Loading evaluation file: reports/caption_full_comparison_minseok_audio_x_caption_sorted_completeness/multi_run_caption_evaluations.json\n",
      "‚úÖ Original JSON files loaded successfully!\n",
      "\n",
      "üìã Original Caption Data Keys: ['id', 'created', 'model', 'media_info', 'usage', 'chunk_responses']\n",
      "üìã Original Evaluation Data Keys: ['all_runs', 'averaged_results', 'num_runs', 'run_delay', 'ground_truth_model']\n",
      "üìä Number of caption chunks: 159\n",
      "üìä Number of evaluation runs: 3\n"
     ]
    }
   ],
   "source": [
    "# Load original JSON files\n",
    "def load_original_json_files():\n",
    "    \"\"\"Load original VLM caption files and GPT-4o evaluation files.\"\"\"\n",
    "    \n",
    "    # Load VLM caption file for the test model and video\n",
    "    caption_file = Path(CAPTIONS_DIR) / TEST_MODEL_NAME / TEST_VIDEO_ID / f\"vlm_captions_{TEST_VIDEO_ID}.json\"\n",
    "    \n",
    "    if not caption_file.exists():\n",
    "        raise FileNotFoundError(f\"Caption file not found: {caption_file}\")\n",
    "    \n",
    "    print(f\"üìÑ Loading caption file: {caption_file}\")\n",
    "    with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "        caption_data = json.load(f)\n",
    "    \n",
    "    # Load GPT-4o evaluation results (multi-run file)\n",
    "    eval_file = Path(EVALUATIONS_DIR) / \"multi_run_caption_evaluations.json\"\n",
    "    \n",
    "    if not eval_file.exists():\n",
    "        raise FileNotFoundError(f\"Evaluation file not found: {eval_file}\")\n",
    "    \n",
    "    print(f\"üìÑ Loading evaluation file: {eval_file}\")\n",
    "    with open(eval_file, 'r', encoding='utf-8') as f:\n",
    "        eval_data = json.load(f)\n",
    "    \n",
    "    print(f\"‚úÖ Original JSON files loaded successfully!\")\n",
    "    \n",
    "    return caption_data, eval_data\n",
    "\n",
    "# Load the original JSON data\n",
    "original_captions, original_evaluations = load_original_json_files()\n",
    "\n",
    "print(f\"\\nüìã Original Caption Data Keys: {list(original_captions.keys())}\")\n",
    "print(f\"üìã Original Evaluation Data Keys: {list(original_evaluations.keys())}\")\n",
    "print(f\"üìä Number of caption chunks: {len(original_captions.get('chunk_responses', []))}\")\n",
    "print(f\"üìä Number of evaluation runs: {len(original_evaluations.get('all_runs', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd5f943",
   "metadata": {},
   "source": [
    "## üîç Extract Data from CSV Files\n",
    "\n",
    "Extract the specific data point from CSV files using the test parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f6c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting data from CSV files...\n",
      "\n",
      "üìÑ CSV Results:\n",
      "   ü§ñ VLM Caption: A man is talking about a soccer game and holding a soccer ball.\n",
      "   üß† GPT-4o Judgment: The test caption is severely lacking in completeness. It only covers a small part of the ground truth, specifically the man holding a soccer ball and speaking. It omits the promotional video, the graphic showing team logos, the wide shot of the soccer field, and the players warming up.\n",
      "   üìä GPT-4o Score: 3\n"
     ]
    }
   ],
   "source": [
    "def extract_from_csv(video_id: str, chunk_id: int, model_name: str) -> Tuple[Optional[str], Optional[str], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Extract VLM caption and GPT-4o evaluation data from CSV files.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (vlm_caption, gpt4o_judgment, gpt4o_score)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter data for the specific video and chunk\n",
    "    caption_row = captions_df[\n",
    "        (captions_df['video_id'] == video_id) & \n",
    "        (captions_df['chunk_id'] == chunk_id)\n",
    "    ]\n",
    "    \n",
    "    eval_row = evaluations_df[\n",
    "        (evaluations_df['video_id'] == video_id) & \n",
    "        (evaluations_df['chunk_id'] == chunk_id)\n",
    "    ]\n",
    "    \n",
    "    if caption_row.empty:\n",
    "        print(f\"‚ùå No caption data found in CSV for {video_id}, chunk {chunk_id}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    if eval_row.empty:\n",
    "        print(f\"‚ùå No evaluation data found in CSV for {video_id}, chunk {chunk_id}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Extract VLM caption\n",
    "    vlm_caption = caption_row.iloc[0][model_name] if model_name in caption_row.columns else None\n",
    "    \n",
    "    # Extract GPT-4o evaluation (judgment and score)\n",
    "    gpt4o_judgment = eval_row.iloc[0][model_name] if model_name in eval_row.columns else None\n",
    "    gpt4o_score = eval_row.iloc[0][f\"{model_name}_score\"] if f\"{model_name}_score\" in eval_row.columns else None\n",
    "    \n",
    "    return vlm_caption, gpt4o_judgment, gpt4o_score\n",
    "\n",
    "# Extract data from CSV\n",
    "print(f\"üîç Extracting data from CSV files...\")\n",
    "csv_caption, csv_judgment, csv_score = extract_from_csv(TEST_VIDEO_ID, TEST_CHUNK_ID, TEST_MODEL_NAME)\n",
    "\n",
    "print(f\"\\nüìÑ CSV Results:\")\n",
    "print(f\"   ü§ñ VLM Caption: {csv_caption}\")\n",
    "print(f\"   üß† GPT-4o Judgment: {csv_judgment}\")\n",
    "print(f\"   üìä GPT-4o Score: {csv_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146370ad",
   "metadata": {},
   "source": [
    "## üìÇ Extract Data from Original JSON Files\n",
    "\n",
    "Extract the corresponding data from the original JSON files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ed09b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Extracting data from original JSON files...\n",
      "\n",
      "üìÑ JSON Results:\n",
      "   ü§ñ VLM Caption: A man is talking about a soccer game and holding a soccer ball.\n",
      "   üß† GPT-4o Judgment: The test caption is severely lacking in completeness. It only covers a small part of the ground truth, specifically the man holding a soccer ball and speaking. It omits the promotional video, the graphic showing team logos, the wide shot of the soccer field, and the players warming up.\n",
      "   üìä GPT-4o Score: 3\n"
     ]
    }
   ],
   "source": [
    "def extract_from_json(video_id: str, chunk_id: int, model_name: str) -> Tuple[Optional[str], Optional[str], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Extract VLM caption and GPT-4o evaluation data from original JSON files.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (vlm_caption, gpt4o_judgment, gpt4o_score)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract VLM caption from original caption file\n",
    "    vlm_caption = None\n",
    "    chunks = original_captions.get('chunk_responses', [])\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        if chunk.get('chunk_id') == chunk_id:\n",
    "            vlm_caption = chunk.get('content', '')\n",
    "            break\n",
    "    \n",
    "    # Extract GPT-4o evaluation from original evaluation file (first run)\n",
    "    gpt4o_judgment = None\n",
    "    gpt4o_score = None\n",
    "    \n",
    "    if 'all_runs' in original_evaluations and original_evaluations['all_runs']:\n",
    "        first_run = original_evaluations['all_runs'][0]\n",
    "        evaluations = first_run.get('evaluations', [])\n",
    "        \n",
    "        for eval_data in evaluations:\n",
    "            if (eval_data.get('video_id') == video_id and \n",
    "                eval_data.get('chunk_index') == chunk_id and \n",
    "                eval_data.get('model_name') == model_name):\n",
    "                \n",
    "                gpt4o_judgment = eval_data.get('judgment', '')\n",
    "                gpt4o_score = eval_data.get('score', eval_data.get('overall_score'))\n",
    "                break\n",
    "    \n",
    "    return vlm_caption, gpt4o_judgment, gpt4o_score\n",
    "\n",
    "# Extract data from original JSON files\n",
    "print(f\"üîç Extracting data from original JSON files...\")\n",
    "json_caption, json_judgment, json_score = extract_from_json(TEST_VIDEO_ID, TEST_CHUNK_ID, TEST_MODEL_NAME)\n",
    "\n",
    "print(f\"\\nüìÑ JSON Results:\")\n",
    "print(f\"   ü§ñ VLM Caption: {json_caption}\")\n",
    "print(f\"   üß† GPT-4o Judgment: {json_judgment}\")\n",
    "print(f\"   üìä GPT-4o Score: {json_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce3378",
   "metadata": {},
   "source": [
    "## ‚öñÔ∏è Compare and Verify Data\n",
    "\n",
    "Perform detailed comparison between CSV and JSON data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64e75d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öñÔ∏è Comparing data for:\n",
      "   Video: 74GSMhR6oI0\n",
      "   Chunk: 0\n",
      "   Model: vila-1.5\n",
      "üîç DETAILED COMPARISON\n",
      "============================================================\n",
      "\n",
      "üìù VLM CAPTION COMPARISON:\n",
      "   CSV:  'A man is talking about a soccer game and holding a soccer ball.'\n",
      "   JSON: 'A man is talking about a soccer game and holding a soccer ball.'\n",
      "   ‚úÖ Match: True\n",
      "\n",
      "üß† GPT-4O JUDGMENT COMPARISON:\n",
      "   CSV:  'The test caption is severely lacking in completeness. It only covers a small part of the ground truth, specifically the man holding a soccer ball and speaking. It omits the promotional video, the graphic showing team logos, the wide shot of the soccer field, and the players warming up.'\n",
      "   JSON: 'The test caption is severely lacking in completeness. It only covers a small part of the ground truth, specifically the man holding a soccer ball and speaking. It omits the promotional video, the graphic showing team logos, the wide shot of the soccer field, and the players warming up.'\n",
      "   ‚úÖ Match: True\n",
      "\n",
      "üìä GPT-4O SCORE COMPARISON:\n",
      "   CSV:  3\n",
      "   JSON: 3\n",
      "   ‚úÖ Match: True\n"
     ]
    }
   ],
   "source": [
    "def compare_data(csv_data: Tuple, json_data: Tuple, test_params: Dict[str, Any]) -> Dict[str, bool]:\n",
    "    \"\"\"\n",
    "    Compare data from CSV and JSON sources.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with comparison results for each data type\n",
    "    \"\"\"\n",
    "    \n",
    "    csv_caption, csv_judgment, csv_score = csv_data\n",
    "    json_caption, json_judgment, json_score = json_data\n",
    "    \n",
    "    comparison_results = {\n",
    "        'caption_match': False,\n",
    "        'judgment_match': False,\n",
    "        'score_match': False,\n",
    "        'all_match': False\n",
    "    }\n",
    "    \n",
    "    print(f\"üîç DETAILED COMPARISON\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Compare VLM Caption\n",
    "    print(f\"\\nüìù VLM CAPTION COMPARISON:\")\n",
    "    print(f\"   CSV:  {repr(csv_caption)}\")\n",
    "    print(f\"   JSON: {repr(json_caption)}\")\n",
    "    \n",
    "    caption_match = csv_caption == json_caption\n",
    "    comparison_results['caption_match'] = caption_match\n",
    "    print(f\"   ‚úÖ Match: {caption_match}\" if caption_match else f\"   ‚ùå Mismatch: {caption_match}\")\n",
    "    \n",
    "    # Compare GPT-4o Judgment\n",
    "    print(f\"\\nüß† GPT-4O JUDGMENT COMPARISON:\")\n",
    "    print(f\"   CSV:  {repr(csv_judgment)}\")\n",
    "    print(f\"   JSON: {repr(json_judgment)}\")\n",
    "    \n",
    "    judgment_match = csv_judgment == json_judgment\n",
    "    comparison_results['judgment_match'] = judgment_match\n",
    "    print(f\"   ‚úÖ Match: {judgment_match}\" if judgment_match else f\"   ‚ùå Mismatch: {judgment_match}\")\n",
    "    \n",
    "    # Compare GPT-4o Score\n",
    "    print(f\"\\nüìä GPT-4O SCORE COMPARISON:\")\n",
    "    print(f\"   CSV:  {csv_score}\")\n",
    "    print(f\"   JSON: {json_score}\")\n",
    "    \n",
    "    # Handle floating point comparison\n",
    "    if csv_score is not None and json_score is not None:\n",
    "        try:\n",
    "            score_match = abs(float(csv_score) - float(json_score)) < 1e-10\n",
    "        except (ValueError, TypeError):\n",
    "            score_match = csv_score == json_score\n",
    "    else:\n",
    "        score_match = csv_score == json_score\n",
    "    \n",
    "    comparison_results['score_match'] = score_match\n",
    "    print(f\"   ‚úÖ Match: {score_match}\" if score_match else f\"   ‚ùå Mismatch: {score_match}\")\n",
    "    \n",
    "    # Overall match\n",
    "    all_match = all([caption_match, judgment_match, score_match])\n",
    "    comparison_results['all_match'] = all_match\n",
    "    \n",
    "    return comparison_results\n",
    "\n",
    "# Perform the comparison\n",
    "print(f\"‚öñÔ∏è Comparing data for:\")\n",
    "print(f\"   Video: {TEST_VIDEO_ID}\")\n",
    "print(f\"   Chunk: {TEST_CHUNK_ID}\")\n",
    "print(f\"   Model: {TEST_MODEL_NAME}\")\n",
    "\n",
    "csv_data = (csv_caption, csv_judgment, csv_score)\n",
    "json_data = (json_caption, json_judgment, json_score)\n",
    "test_params = {\n",
    "    'video_id': TEST_VIDEO_ID,\n",
    "    'chunk_id': TEST_CHUNK_ID,\n",
    "    'model_name': TEST_MODEL_NAME\n",
    "}\n",
    "\n",
    "comparison_results = compare_data(csv_data, json_data, test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99dfb75",
   "metadata": {},
   "source": [
    "## üìã Final Verification Results\n",
    "\n",
    "Display the final verification status and recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cab1bc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ FINAL VERIFICATION RESULTS\n",
      "============================================================\n",
      "\n",
      "‚úÖ OVERALL STATUS: PASSED\n",
      "\n",
      "üìä Individual Results:\n",
      "   üìù VLM Caption:     ‚úÖ MATCH\n",
      "   üß† GPT-4o Judgment: ‚úÖ MATCH\n",
      "   üìä GPT-4o Score:    ‚úÖ MATCH\n",
      "\n",
      "üéâ SUCCESS: CSV files are correctly generated!\n",
      "   The data in CSV files perfectly matches the original JSON sources.\n",
      "   You can confidently use the CSV files for your analysis.\n",
      "\n",
      "üí° RECOMMENDATIONS:\n",
      "   ‚Ä¢ CSV files are ready for use\n",
      "   ‚Ä¢ Test additional data points to ensure consistency\n",
      "   ‚Ä¢ Consider spot-checking random samples for extra validation\n",
      "\n",
      "üîÑ ADDITIONAL TESTING:\n",
      "To test more data points, modify the test parameters in cell 3 and re-run the notebook:\n",
      "   ‚Ä¢ Try different video_ids: ['74GSMhR6oI0', 'Mhs73xQWo5g', 'WnzPCvaxYvs', 'WuFL2bJm2yo', 'aSHaM2GcjXY', 'hwxQXfHgLhI']\n",
      "   ‚Ä¢ Try different chunk_ids: 0 to max_chunks_per_video\n",
      "   ‚Ä¢ Try different models: vila-1.5, nvila, cosmos_reason1, qwen3-vl-30b-a3b-instruct\n"
     ]
    }
   ],
   "source": [
    "# Display final verification results\n",
    "print(f\"\\nüéØ FINAL VERIFICATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "def display_verification_status(results: Dict[str, bool]):\n",
    "    \"\"\"Display verification results in a formatted way.\"\"\"\n",
    "    \n",
    "    status_icon = \"‚úÖ\" if results['all_match'] else \"‚ùå\"\n",
    "    \n",
    "    print(f\"\\n{status_icon} OVERALL STATUS: {'PASSED' if results['all_match'] else 'FAILED'}\")\n",
    "    print(f\"\\nüìä Individual Results:\")\n",
    "    print(f\"   üìù VLM Caption:     {'‚úÖ MATCH' if results['caption_match'] else '‚ùå MISMATCH'}\")\n",
    "    print(f\"   üß† GPT-4o Judgment: {'‚úÖ MATCH' if results['judgment_match'] else '‚ùå MISMATCH'}\")\n",
    "    print(f\"   üìä GPT-4o Score:    {'‚úÖ MATCH' if results['score_match'] else '‚ùå MISMATCH'}\")\n",
    "    \n",
    "    if results['all_match']:\n",
    "        print(f\"\\nüéâ SUCCESS: CSV files are correctly generated!\")\n",
    "        print(f\"   The data in CSV files perfectly matches the original JSON sources.\")\n",
    "        print(f\"   You can confidently use the CSV files for your analysis.\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: Data mismatch detected!\")\n",
    "        print(f\"   There are differences between CSV and JSON sources.\")\n",
    "        print(f\"   Please check the CSV generation script or data processing logic.\")\n",
    "    \n",
    "    print(f\"\\nüí° RECOMMENDATIONS:\")\n",
    "    if results['all_match']:\n",
    "        print(f\"   ‚Ä¢ CSV files are ready for use\")\n",
    "        print(f\"   ‚Ä¢ Test additional data points to ensure consistency\")\n",
    "        print(f\"   ‚Ä¢ Consider spot-checking random samples for extra validation\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Investigate the cause of data mismatches\")\n",
    "        print(f\"   ‚Ä¢ Check CSV generation logic in generate_csv_from_data.py\")\n",
    "        print(f\"   ‚Ä¢ Verify data alignment between sources\")\n",
    "        print(f\"   ‚Ä¢ Re-run CSV generation if necessary\")\n",
    "\n",
    "# Display the verification status\n",
    "display_verification_status(comparison_results)\n",
    "\n",
    "# Additional testing suggestion\n",
    "print(f\"\\nüîÑ ADDITIONAL TESTING:\")\n",
    "print(f\"To test more data points, modify the test parameters in cell 3 and re-run the notebook:\")\n",
    "print(f\"   ‚Ä¢ Try different video_ids: {list(captions_df['video_id'].unique())}\")\n",
    "print(f\"   ‚Ä¢ Try different chunk_ids: 0 to max_chunks_per_video\")\n",
    "print(f\"   ‚Ä¢ Try different models: vila-1.5, nvila, cosmos_reason1, qwen3-vl-30b-a3b-instruct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d923f7f",
   "metadata": {},
   "source": [
    "## üöÄ Quick Batch Testing (Optional)\n",
    "\n",
    "Run this cell to test multiple random data points for comprehensive validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f1b9ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running 5 random verification tests...\n",
      "============================================================\n",
      "\n",
      "üß™ Test 1: WnzPCvaxYvs, chunk 56, cosmos_reason1\n",
      "   ‚úÖ PASSED\n",
      "\n",
      "üß™ Test 2: WuFL2bJm2yo, chunk 46, nvila\n",
      "   ‚úÖ PASSED\n",
      "\n",
      "üß™ Test 3: 74GSMhR6oI0, chunk 56, vila-1.5\n",
      "   ‚úÖ PASSED\n",
      "\n",
      "üß™ Test 4: WuFL2bJm2yo, chunk 39, cosmos_reason1\n",
      "   ‚úÖ PASSED\n",
      "\n",
      "üß™ Test 5: Mhs73xQWo5g, chunk 215, qwen3-vl-30b-a3b-instruct\n",
      "   ‚úÖ PASSED\n",
      "\n",
      "üìä BATCH TEST SUMMARY:\n",
      "   Total: 5\n",
      "   Passed: 5 ‚úÖ\n",
      "   Failed: 0 ‚ùå\n",
      "   Success Rate: 100.0%\n",
      "üí° Uncomment the lines above to run batch testing of 5 random data points.\n"
     ]
    }
   ],
   "source": [
    "# Quick batch testing (optional)\n",
    "def quick_batch_test(num_tests: int = 5) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Test multiple random data points for comprehensive validation.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with test statistics\n",
    "    \"\"\"\n",
    "    \n",
    "    import random\n",
    "    \n",
    "    # Get available test parameters\n",
    "    available_videos = list(captions_df['video_id'].unique())\n",
    "    available_models = ['vila-1.5', 'nvila', 'cosmos_reason1', 'qwen3-vl-30b-a3b-instruct']\n",
    "    \n",
    "    results = {\n",
    "        'total_tests': 0,\n",
    "        'passed_tests': 0,\n",
    "        'failed_tests': 0\n",
    "    }\n",
    "    \n",
    "    print(f\"üöÄ Running {num_tests} random verification tests...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for i in range(num_tests):\n",
    "        # Select random parameters\n",
    "        video_id = random.choice(available_videos)\n",
    "        model_name = random.choice(available_models)\n",
    "        \n",
    "        # Get available chunks for this video\n",
    "        video_chunks = captions_df[captions_df['video_id'] == video_id]['chunk_id'].tolist()\n",
    "        chunk_id = random.choice(video_chunks)\n",
    "        \n",
    "        print(f\"\\nüß™ Test {i+1}: {video_id}, chunk {chunk_id}, {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Load original JSON for this model/video\n",
    "            caption_file = Path(CAPTIONS_DIR) / model_name / video_id / f\"vlm_captions_{video_id}.json\"\n",
    "            if not caption_file.exists():\n",
    "                print(f\"   ‚ö†Ô∏è  Skipping - caption file not found\")\n",
    "                continue\n",
    "                \n",
    "            with open(caption_file, 'r', encoding='utf-8') as f:\n",
    "                caption_data = json.load(f)\n",
    "            \n",
    "            # Extract data from both sources\n",
    "            csv_caption, csv_judgment, csv_score = extract_from_csv(video_id, chunk_id, model_name)\n",
    "            \n",
    "            # Extract from JSON\n",
    "            json_caption = None\n",
    "            chunks = caption_data.get('chunk_responses', [])\n",
    "            for chunk in chunks:\n",
    "                if chunk.get('chunk_id') == chunk_id:\n",
    "                    json_caption = chunk.get('content', '')\n",
    "                    break\n",
    "            \n",
    "            json_judgment = None\n",
    "            json_score = None\n",
    "            if 'all_runs' in original_evaluations and original_evaluations['all_runs']:\n",
    "                first_run = original_evaluations['all_runs'][0]\n",
    "                evaluations = first_run.get('evaluations', [])\n",
    "                \n",
    "                for eval_data in evaluations:\n",
    "                    if (eval_data.get('video_id') == video_id and \n",
    "                        eval_data.get('chunk_index') == chunk_id and \n",
    "                        eval_data.get('model_name') == model_name):\n",
    "                        \n",
    "                        json_judgment = eval_data.get('judgment', '')\n",
    "                        json_score = eval_data.get('score', eval_data.get('overall_score'))\n",
    "                        break\n",
    "            \n",
    "            # Compare\n",
    "            caption_match = csv_caption == json_caption\n",
    "            judgment_match = csv_judgment == json_judgment\n",
    "            \n",
    "            if csv_score is not None and json_score is not None:\n",
    "                try:\n",
    "                    score_match = abs(float(csv_score) - float(json_score)) < 1e-10\n",
    "                except (ValueError, TypeError):\n",
    "                    score_match = csv_score == json_score\n",
    "            else:\n",
    "                score_match = csv_score == json_score\n",
    "            \n",
    "            all_match = all([caption_match, judgment_match, score_match])\n",
    "            \n",
    "            results['total_tests'] += 1\n",
    "            if all_match:\n",
    "                results['passed_tests'] += 1\n",
    "                print(f\"   ‚úÖ PASSED\")\n",
    "            else:\n",
    "                results['failed_tests'] += 1\n",
    "                print(f\"   ‚ùå FAILED (Caption: {caption_match}, Judgment: {judgment_match}, Score: {score_match})\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Error: {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run batch testing (uncomment the next line to run)\n",
    "batch_results = quick_batch_test(5)\n",
    "print(f\"\\nüìä BATCH TEST SUMMARY:\")\n",
    "print(f\"   Total: {batch_results['total_tests']}\")\n",
    "print(f\"   Passed: {batch_results['passed_tests']} ‚úÖ\")\n",
    "print(f\"   Failed: {batch_results['failed_tests']} ‚ùå\")\n",
    "if batch_results['total_tests'] > 0:\n",
    "    success_rate = (batch_results['passed_tests'] / batch_results['total_tests']) * 100\n",
    "    print(f\"   Success Rate: {success_rate:.1f}%\")\n",
    "\n",
    "print(\"üí° Uncomment the lines above to run batch testing of 5 random data points.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd9185",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm_evaluation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
